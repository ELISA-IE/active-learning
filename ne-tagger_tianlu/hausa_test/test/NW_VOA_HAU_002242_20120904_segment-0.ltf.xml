<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT>
  <DOC id="NW_VOA_HAU_002242_20120904_segment-0">
    <TEXT><SEG id="segment-0" start_char="0" end_char="225">
<ORIGINAL_TEXT>Manyan jamiÊ¼an Majalisar Dinkin Duniya sun jadada bukatar kara maida hankali a yunkurin kawar da cutar kanjamau, suka kuma ce ana bukatar musamman a fadada ayyukan yaki da cutar da kuma kudaden da ake kashewa a wannan yumurin.</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="0" end_char="5">Manyan</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="7" end_char="13">jamiÊ¼an</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="15" end_char="23">Majalisar</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="25" end_char="30">Dinkin</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="32" end_char="37">Duniya</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="39" end_char="41">sun</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="43" end_char="48">jadada</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="50" end_char="56">bukatar</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="58" end_char="61">kara</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="63" end_char="67">maida</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="69" end_char="75">hankali</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="77" end_char="77">a</TOKEN>
<TOKEN id="token-0-12" pos="word" morph="none" start_char="79" end_char="86">yunkurin</TOKEN>
<TOKEN id="token-0-13" pos="word" morph="none" start_char="88" end_char="92">kawar</TOKEN>
<TOKEN id="token-0-14" pos="word" morph="none" start_char="94" end_char="95">da</TOKEN>
<TOKEN id="token-0-15" pos="word" morph="none" start_char="97" end_char="101">cutar</TOKEN>
<TOKEN id="token-0-16" pos="word" morph="none" start_char="103" end_char="110">kanjamau</TOKEN>
<TOKEN id="token-0-17" pos="punct" morph="none" start_char="111" end_char="111">,</TOKEN>
<TOKEN id="token-0-18" pos="word" morph="none" start_char="113" end_char="116">suka</TOKEN>
<TOKEN id="token-0-19" pos="word" morph="none" start_char="118" end_char="121">kuma</TOKEN>
<TOKEN id="token-0-20" pos="word" morph="none" start_char="123" end_char="124">ce</TOKEN>
<TOKEN id="token-0-21" pos="word" morph="none" start_char="126" end_char="128">ana</TOKEN>
<TOKEN id="token-0-22" pos="word" morph="none" start_char="130" end_char="136">bukatar</TOKEN>
<TOKEN id="token-0-23" pos="word" morph="none" start_char="138" end_char="145">musamman</TOKEN>
<TOKEN id="token-0-24" pos="word" morph="none" start_char="147" end_char="147">a</TOKEN>
<TOKEN id="token-0-25" pos="word" morph="none" start_char="149" end_char="154">fadada</TOKEN>
<TOKEN id="token-0-26" pos="word" morph="none" start_char="156" end_char="162">ayyukan</TOKEN>
<TOKEN id="token-0-27" pos="word" morph="none" start_char="164" end_char="167">yaki</TOKEN>
<TOKEN id="token-0-28" pos="word" morph="none" start_char="169" end_char="170">da</TOKEN>
<TOKEN id="token-0-29" pos="word" morph="none" start_char="172" end_char="176">cutar</TOKEN>
<TOKEN id="token-0-30" pos="word" morph="none" start_char="178" end_char="179">da</TOKEN>
<TOKEN id="token-0-31" pos="word" morph="none" start_char="181" end_char="184">kuma</TOKEN>
<TOKEN id="token-0-32" pos="word" morph="none" start_char="186" end_char="192">kudaden</TOKEN>
<TOKEN id="token-0-33" pos="word" morph="none" start_char="194" end_char="195">da</TOKEN>
<TOKEN id="token-0-34" pos="word" morph="none" start_char="197" end_char="199">ake</TOKEN>
<TOKEN id="token-0-35" pos="word" morph="none" start_char="201" end_char="207">kashewa</TOKEN>
<TOKEN id="token-0-36" pos="word" morph="none" start_char="209" end_char="209">a</TOKEN>
<TOKEN id="token-0-37" pos="word" morph="none" start_char="211" end_char="216">wannan</TOKEN>
<TOKEN id="token-0-38" pos="word" morph="none" start_char="218" end_char="224">yumurin</TOKEN>
<TOKEN id="token-0-39" pos="punct" morph="none" start_char="225" end_char="225">.</TOKEN>
</SEG>
</TEXT>
  </DOC>
</LCTL_TEXT>
